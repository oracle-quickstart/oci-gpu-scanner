{{- if .Values.nodeProblemDetector.enabled }}
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: oke-node-problem-detector
  namespace: {{ .Values.nodeProblemDetector.namespace | default "kube-system" }}
  labels:
    app: oke-node-problem-detector
    component: gpu-monitoring
    {{- with .Values.global.labels }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
spec:
  selector:
    matchLabels:
      app: oke-node-problem-detector
  template:
    metadata:
      labels:
        app: oke-node-problem-detector
        component: gpu-monitoring
    spec:
      nodeSelector:
        {{- if .Values.nodeProblemDetector.nodeSelector }}
        {{- toYaml .Values.nodeProblemDetector.nodeSelector | nindent 8 }}
        {{- else }}
        oci.oraclecloud.com/oke-node-problem-detector-enabled: "true"
        {{- end }}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/os
                    operator: In
                    values:
                      - linux
      containers:
        - args:
            - /node-problem-detector --logtostderr --prometheus-port=${PROMETHEUS_PORT}
              --prometheus-address 0.0.0.0 --config.system-log-monitor=/config/kernel-monitor.json,/config/readonly-monitor.json
              {{- if .Values.nodeProblemDetector.enableGpuChecks }}
              --config.custom-plugin-monitor=/node-problem-detector-gpu-check/dr-hpc.json
              {{- end }}
              --enable-k8s-exporter=true
          command:
            - /bin/sh
            - -c
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: PROMETHEUS_PORT
              value: {{ .Values.nodeProblemDetector.prometheusPort | default "20257" | quote }}
          image: {{ .Values.nodeProblemDetector.image.repository }}:{{ .Values.nodeProblemDetector.image.tag }}@{{ .Values.nodeProblemDetector.image.sha256 }}
          imagePullPolicy: {{ .Values.nodeProblemDetector.image.pullPolicy | default "Always" }}
          name: oke-node-problem-detector
          ports:
            - containerPort: {{ .Values.nodeProblemDetector.prometheusPort | default 20257 }}
              name: metrics
              protocol: TCP
          resources:
            {{- toYaml .Values.nodeProblemDetector.resources | nindent 12 }}
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: /var/log
              name: log
              readOnly: true
            - mountPath: /dev/kmsg
              name: kmsg
              readOnly: true
            - mountPath: /run/systemd/system
              name: systemd
            - mountPath: /var/run/dbus/
              mountPropagation: Bidirectional
              name: dbus
            - mountPath: /node-problem-detector-custom-check
              name: node-problem-detector-custom-check
              readOnly: true
            {{- if .Values.nodeProblemDetector.enableGpuChecks }}
            - mountPath: /node-problem-detector-gpu-check
              name: node-problem-detector-gpu-check
              readOnly: true
            {{- end }}
      serviceAccountName: oke-node-problem-detector-sa
      tolerations:
        {{- toYaml .Values.nodeProblemDetector.tolerations | nindent 8 }}
      volumes:
        - hostPath:
            path: {{ .Values.nodeProblemDetector.drhpcResultsPath | default "/var/lib/oci-dr-hpc-v2" }}
          name: log
        - hostPath:
            path: /dev/kmsg
          name: kmsg
        - hostPath:
            path: /run/systemd/system/
          name: systemd
        - hostPath:
            path: /var/run/dbus/
          name: dbus
        - configMap:
            defaultMode: 493
            name: node-problem-detector-custom-check
          name: node-problem-detector-custom-check
        {{- if .Values.nodeProblemDetector.enableGpuChecks }}
        - configMap:
            defaultMode: 493
            name: node-problem-detector-gpu-check
          name: node-problem-detector-gpu-check
        {{- end }}

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-problem-detector-custom-check
  namespace: {{ .Values.nodeProblemDetector.namespace | default "kube-system" }}
data:
  imds_reachability.sh: |
    #!/bin/bash   
    URLS=(
      "http://169.254.169.254/opc/v2/identity/"
      "http://169.254.169.254/opc/v2/instance/"
      "http://169.254.169.254/opc/v2/vnics/"
    )
    
    CHECK_RESULT=0
    
    for url in "${URLS[@]}"; do
      HTTP_STATUS=$(curl -s --fail -H "Authorization: Bearer Oracle" -L0 -o /dev/null -w "%{http_code}" --max-time 10 "$url")
    
      if [[ "$HTTP_STATUS" != "200" ]]; then
        CHECK_RESULT=1
      fi  
    done
    
    if [[ ${CHECK_RESULT} == 0 ]]; then
      echo "IMDS is reachable"
      exit 0
    else
      echo "IMDS is unreachable"
      exit 1
    fi

  imds_reachability.json: |
    {
      "plugin": "custom",
      "pluginConfig": {
        "invokeInterval": "30s",
        "timeout": "10s",
        "maxOutputLength": 80,
        "consecutiveFailureCount": 1
      },
      "source": "imds-reachability",
      "conditions": [
        {
          "type": "IMDSUnreachable",
          "status": "False",
          "reason": "IMDSCheckPassed",
          "message": "IMDS URL is reachable"
        }
      ],
      "rules": [
        {
          "type": "temporary",
          "condition": "IMDSUnreachable",
          "reason": "IMDSCheckFailed",
          "message": "IMDS URL is unreachable",
          "path": "/node-problem-detector-custom-check/imds_reachability.sh"
        }
      ]
    }

{{- if .Values.nodeProblemDetector.enableGpuChecks }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-problem-detector-gpu-check
  namespace: {{ .Values.nodeProblemDetector.namespace | default "kube-system" }}
data:
  dr_hpc_check.sh: |
    #!/bin/bash

    readonly OK=0
    readonly NONOK=1

    check_type=$1
    
    latest_remediation_json=$(ls -t /var/log/remediation_*.json 2>/dev/null | head -n 1)
    
    ERROR_MSG=$(jq -r --arg check_type "$check_type" '
      if (.critical_issues // 0) == 0 then
        empty
      else
        .recommendations[]
        | select(.type=="critical" and .test_name==$check_type)
        | "\(.fault_code): \(.issue)"
      end
    ' "${latest_remediation_json}")
    
    if [ -n "$ERROR_MSG" ]; then
      echo "$ERROR_MSG"
      exit $NONOK
    else
      echo "No issues detected"
      exit $OK
    fi

  dr-hpc.json: |
{{ (.Files.Get "files/npd/dr-hpc.json") | nindent 4 }}
{{- end }}

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: oke-node-problem-detector-sa
  namespace: {{ .Values.nodeProblemDetector.namespace | default "kube-system" }}

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: oke-npd-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:node-problem-detector
subjects:
  - kind: ServiceAccount
    name: oke-node-problem-detector-sa
    namespace: {{ .Values.nodeProblemDetector.namespace | default "kube-system" }}
{{- end }}